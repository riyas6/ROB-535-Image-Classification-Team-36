{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea842dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import transforms\n",
    "import engine\n",
    "import coco_utils\n",
    "import coco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val image filtering list\n",
    "images, labels = [], []\n",
    "for folder in glob.glob('C:\\\\Users\\\\riyas\\\\Desktop\\\\Train_data_bbox/*'):\n",
    "    for file in glob.glob('{}/*_bbox2D.bin'.format(folder)):\n",
    "        a = file.split(\".\")[0][:-7]\n",
    "        images.append(f\"{a}_image.jpg\")\n",
    "        labels.append(f\"{a}_bbox2D.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41591926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image filtering list\n",
    "images_test, labels_test = [], []\n",
    "for folder in glob.glob('C:\\\\Users\\\\riyas\\\\Desktop\\\\test/*'):\n",
    "    for file in glob.glob('{}/*_image.jpg'.format(folder)):\n",
    "        a = file.split(\".\")[0][:-5]\n",
    "        images_test.append(f\"{a}image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f5328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To test if all the data is loaded properly\n",
    "len(images), len(labels), len(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test data loading\n",
    "class ImageDataset2(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transforms=None):\n",
    "        self.images_list = images\n",
    "        self.transforms = transforms\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images_list[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\") \n",
    " \n",
    "        if self.transforms is not None:\n",
    "            img, _ = self.transforms(img, [])\n",
    " \n",
    "        return img, _\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train data loading\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transforms=None):\n",
    "        self.images_list = images\n",
    "        self.labels_list = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # load all image files, sorting them to ensure that they are aligned\n",
    "        self.label_data_types = []\n",
    "        for i in range(11):\n",
    "            self.label_data_types.append((f\"temp{i}\", np.float32))\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and bbox\n",
    "        img_path = self.images_list[idx]\n",
    "        bbox_path = self.labels_list[idx]\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\") \n",
    "        labels = np.fromfile(bbox_path, dtype=np.float32)\n",
    "        \n",
    "        # Assign bounding box coordinates and labels from bbox2D files\n",
    "        x_min, x_max = labels[0],labels[2]\n",
    "        y_min, y_max = labels[1],labels[3]\n",
    "        label = [np.int64(labels[4])]\n",
    "        boxes = [[np.int32(x_min), np.int32(y_min), np.int32(x_max), np.int32(y_max)]]\n",
    "        \n",
    "        objects = [1]\n",
    " \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    " \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n",
    " \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    " \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    " \n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            if \"masks\" in target:\n",
    "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "            if \"keypoints\" in target:\n",
    "                keypoints = target[\"keypoints\"]\n",
    "                keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
    "                target[\"keypoints\"] = keypoints\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeee993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    " \n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # 50% chance of flipping horizontally\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    " \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8e188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "start = time.time()\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 3\n",
    "\n",
    "dataset = ImageDataset(images, labels, get_transform(train=True))\n",
    "dataset_test = ImageDataset2(images_test, labels_test, get_transform(train=False))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset_test, [i for i in range(len(dataset_test))])\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, # num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, # num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)  # Or get_object_detection_model(num_classes)\n",
    "\n",
    "# change to the model\n",
    "model.head.classification_head.module_list = torch.nn.ModuleList([\n",
    "    torch.nn.Conv2d(512, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "    torch.nn.Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "])\n",
    "\n",
    "model.head.regression_head.module_list = torch.nn.ModuleList([\n",
    "    torch.nn.Conv2d(512, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "    torch.nn.Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "])\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# SGD\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "# cos learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "\n",
    "# let's train it for   epochs\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    # Engine.py  train_one_epoch function takes both images and targets. to(device)\n",
    "    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=500)\n",
    "    \n",
    "    print(\"Epoch done!\")\n",
    "    \n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on the test dataset    \n",
    "    #evaluate(model, val_data_loader, device=device)\n",
    "    \n",
    "    torch.save(model, 'this_model_epoch' + str(epoch) + '.pth')\n",
    "    \n",
    "    print('')\n",
    "    print('==================================================')\n",
    "    print('')\n",
    "end = time.time()\n",
    "print(\"That's it!\")\n",
    "print(\"The time of execution of above program is :\", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad7f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def new_evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "    \n",
    "    csv_name = 'outputs_final.csv'\n",
    "    row = [None, None]\n",
    "    f = open(csv_name, 'w', newline='')\n",
    "    f.write('guid/image,label\\n' )\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    j = 0  \n",
    "    \n",
    "    for images, target in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "       \n",
    "        a1 = outputs[0]['labels'].cpu().detach().numpy()\n",
    "        a2 = outputs[0]['scores'].cpu().detach().numpy()\n",
    "        text12 = images_test[j].split('\\\\')[-2] + '/' + images_test[j].split('\\\\')[-1].split('.')[0]\n",
    "        \n",
    "        row[0] = text12[:-6]\n",
    "        row[1] = str(a1[np.argmax(a2)])\n",
    "        writer.writerow(row)\n",
    "        print(\"Predicted Class\", row[1])\n",
    "        \n",
    "        j += 1\n",
    "\n",
    "    f.close()\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ce32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model39 = torch.load('this_model_epoch39.pth')\n",
    "new_evaluate(model39, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b87c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
